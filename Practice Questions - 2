=============================================================================================
empRDD1 = sc.parallelize([(1001, 'Big Data'), (1002, 'Java'), (1003, 'SQL')])

result1 = empRDD1.collectAsMap()

print(result1)  #  {1001: 'Big Data', 1002: 'Java', 1003: 'SQL'}
=============================================================================================
empRDD2 = sc.parallelize(["1001, John", "1002, Jai", "1003, Sam", "1003, Kim"])

result2 = empRDD2.map(lambda x: int(x.split(',')[0]))  #  [1001, 1002, 1003, 1003]
            .distinct().collect()

print(result2)  #  [1001, 1002, 1003]

result2 = empRDD2.map(lambda x: x.split(',')).collect() #[['1001', ' John'], ['1002', ' Jai'], ['1003', ' Sam'], ['1003', ' Kim']]
=============================================================================================
dataRDD = (sc.parallelize(["C, 29", "B, 9", "A, 18"])
             .map(lambda x: x.split(',')).sortBy(lambda x: x[1])
             .collect())

print(dataRDD)  #  [['A', ' 18'], ['C', ' 29'], ['B', ' 9']]


rdd = sc.parallelize(["user1,password1","user2,password2", "user2,password2","user4,password1","user1,password1"])

finalrdd = rdd.map(lambda x: x.split(',')[0]).map(lambda x: (x, 1)).reduceByKey(lambda x,y: x+y).collect()

print(finalrdd)  #  [('user1', 2), ('user2', 2), ('user4', 1)]
=============================================================================================
rdd = sc.parallelize(["hello","welcome","all","sudheer"])

print(rdd.first())  # hello

print(rdd.count())  # 4

print(rdd.take(1))  # ['hello']

print(rdd.collect()) # prints entire rdd

print(rdd.filter(lambda x:x[0] == 'H').collect()) # []
=============================================================================================
def fun(x):
    return x.split(",")

rdd = sc.parallelize(["1,s,d","2,3,r","3,f,5","4,d,5"])

newrdd = rdd.map(fun)

print(newrdd.first()) #  ['1', 's', 'd']
=============================================================================================
rdd = sc.parallelize([
    ("1001", "John", "Finance", "20000"),
    ("1002", "Harry", "Finance", ""),
    ("1003", "Marry", "Finance", "30000"),
    ("1004", "Jim", "Finance", "")])

msal = rdd.map(lambda x: (x[0], x[3])).collect()  #  [('1001', '20000'), ('1002', ''), ('1003', '30000'), ('1004', '')]

msal1 = rdd.map(lambda x: (x[0], x[3])).filter(lambda x: x[1] == "").collect()  # [('1002', ''), ('1004', '')]
# msal2 = rdd.map(lambda x: (x[0], x[3])).filter(lambda x: x[1] <> "").collect() # invalid syntax
msal3 = rdd.map(lambda x: (x[0], x[3])).filter(lambda x: x[1] == "null").collect() # []
msal4 = rdd.map(lambda x: (x[0], x[3])).filter(lambda x: len(x[1]) == 0).collect() # [('1002', ''), ('1004', '')]

print(msal)
print(msal1)
# print(msal2)
print(msal3)
print(msal4)
=============================================================================================
rdd = sc.parallelize([
    ("USA", 8000, "EAST"),
    ("USA", 3000, "WEST"),
    ("India", 8000, "EAST"),
    ("India", 9000, "WEST")
])

print(rdd.map(lambda x: (x[1],x[2])).filter(lambda x: x[1] == "EAST").map(lambda x: x[0]).sum())

from operator import add

total_sales_east = rdd.filter(lambda x: x[2] == "EAST").map(lambda x: x[1]).reduce(add)

print(total_sales_east)
=============================================================================================
rdd = sc.textFile("file1.txt")
#  count of records in a file
print(rdd.countByKey()) # defaultdict(<class 'int'>, {'0': 21035})

print(rdd.count()) # 21035
=============================================================================================
dataset = [("John", "Williams", "India", 5000),
           ("Sah", "Roy", "USA", 8000),
           ("Harry", "Williams", "India", 4000),
           ("Marry", "Jones", "USA", 10000)]

salesschema = ["employee_name", "employee_surname", "location", "profit"]

df = spark.createDataFrame(data=dataset, schema=salesschema)

df.show()

result = df.groupBy("location").sum("profit").alias("profit_margin")
result.show()

result1 = df.groupBy("location").sum("profit").withColumnRenamed("sum(profit)","profit_margin")
result1.show()
=============================================================================================
dataset = [("John", "Williams", "India", 5000),
           ("Sah", "Roy", "USA", 8000),
           ("Harry", "Williams", "India", 4000),
           ("Marry", "Jones", "USA", 10000)]

salesschema = ["employee_name", "employee_surname", "location", "profit"]

df = spark.createDataFrame(data=dataset, schema=salesschema)

df.select("employee_name", "employee_surname").show()

df.drop("location", "profit").show()
=============================================================================================
rdd = sc.parallelize(range(5, 7)).map(lambda x: (x, x*x*x))

df = spark.createDataFrame(rdd).toDF("Key", "Cube")

df.show()

+---+----+
|Key|Cube|
+---+----+
|  5| 125|
|  6| 216|
+---+----+
=============================================================================================
dataset = [("Harry", "Williams", "India", 5000),
           ("Sam", "Roy", "USA", 8000),
           ("Jones", "Williams", "India", 4000),
           ("Marry", "Hay", "USA", 10000)]

salesschema = ["employee_name", "employee_surname", "location", "salary"]

df = spark.createDataFrame(dataset, salesschema)

result = df.groupBy("location").sum("salary")

result.show()

+--------+-----------+
|location|sum(salary)|
+--------+-----------+
|   India|       9000|
|     USA|      18000|
+--------+-----------+

result1 = df.groupBy("location").sum("salary").count()

result1.show()

'int' object has no attribute 'show'
=============================================================================================
