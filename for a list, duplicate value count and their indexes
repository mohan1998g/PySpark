# Given list
List = [1, 1, 1, 2, 3, 4, 5, 5, 6, 8, 8]

# Create RDD from the list
rdd = spark.sparkContext.parallelize(enumerate(List))

# Count the occurrences of each value
counts = rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda a, b: a + b)

# Filter values that are repeated (count > 1)
repeated_values = counts.filter(lambda x: x[1] > 1)

# Collect the repeated values
repeated_values_list = repeated_values.collect()

# Find the index numbers of repeated values
index_of_repeats = rdd.filter(lambda x: x[1] in [val[0] for val in repeated_values_list]).groupByKey().mapValues(list)

# Show the result
print("Repeated values with counts:", repeated_values_list)
print("Index numbers of repeated values:", index_of_repeats.collect())
