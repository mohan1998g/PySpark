# Sample data
data = [
        (1, "p1,p2,p3,p4"),
        (2, "p1"),
        (3, "p1,p2"),
        (4, "p1,p2,p3,p4,p5,p6,p7")
]

# Define schema and create DataFrame
columns = ["store", "entries"]
df = spark.createDataFrame(data, schema=columns)

+-----+--------------------+--------------------+
|store|             entries|               cplit|
+-----+--------------------+--------------------+
|    1|         p1,p2,p3,p4|    [p1, p2, p3, p4]|
|    2|                  p1|                [p1]|
|    3|               p1,p2|            [p1, p2]|
|    4|p1,p2,p3,p4,p5,p6,p7|[p1, p2, p3, p4, ...|
+-----+--------------------+--------------------+

splitdf = df.withColumn("cplit", split("entries", ","))
splitdf.show()

+-----+--------------------+--------------------+-----------+
|store|             entries|               cplit|entry_count|
+-----+--------------------+--------------------+-----------+
|    1|         p1,p2,p3,p4|    [p1, p2, p3, p4]|          4|
|    2|                  p1|                [p1]|          1|
|    3|               p1,p2|            [p1, p2]|          2|
|    4|p1,p2,p3,p4,p5,p6,p7|[p1, p2, p3, p4, ...|          7|
+-----+--------------------+--------------------+-----------+

# Split entries and count items
df_with_count = splitdf.withColumn("entry_count", size("cplit"))
df_with_count.show()

+-----+--------------------+--------------------+-----------+
|store|             entries|               cplit|entry_count|
+-----+--------------------+--------------------+-----------+
|    4|p1,p2,p3,p4,p5,p6,p7|[p1, p2, p3, p4, ...|          7|
+-----+--------------------+--------------------+-----------+
                             
# Get the store with the maximum entry count
max_entry_df = df_with_count.orderBy("entry_count", ascending=False).limit(1)
max_entry_df.show()
# Select required columns and show result
max_entry_df.select("store", "entry_count").show()

+-----+-----------+
|store|entry_count|
+-----+-----------+
|    4|          7|
+-----+-----------+

