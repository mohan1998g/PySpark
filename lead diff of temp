data = [(1111, "2021-01-15", 10),
        (1111, "2021-01-16", 15),
        (1111, "2021-01-17", 30),
        (1112, "2021-01-15", 10),
        (1112, "2021-01-15", 20),
        (1112, "2021-01-15", 30)]

myschema = ["sensorid", "timestamp", "values"]

df = spark.createDataFrame(data, schema=myschema)
df.show()

d1 = Window.partitionBy("sensorid").orderBy("values")

leaddf = df.withColumn("nextvalues", lead("values", 1).over(d1))
leaddf.show()

resultdf = (leaddf.filter(col("nextvalues").isNotNull())
            .withColumn("diff", expr("nextvalues-values"))
            .orderBy(col("sensorid")))

resultdf.show()
