# write a sql query to find manager name with atleast 5 reporting employees
# Define the schema
schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("Name", StringType(), True),
        StructField("Dept", StringType(), True),
        StructField("Manager", IntegerType(), True)
])

# Sample data
data = [
        (101, "John", "A", None),
        (102, "Dan", "A", 101),
        (103, "James", "A", 101),
        (104, "Amy", "A", 101),
        (105, "Anne", "A", 101),
        (106, "Ron", "B", 101)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

# Show the DataFrame
df.show()

countdf = df.groupby("Manager").agg(count("Manager").alias("count"))

--------------------------------------------------------------
# first given is input MNGID = EMPID ,we need to get only manager details MNGID
# Define the schema
schema = StructType([
        StructField("EMPID", IntegerType(), True),
        StructField("EMNAME", StringType(), True),
        StructField("MNGID", IntegerType(), True)
])
# first given is input MNGID = EMPID ,we need to get only manager details MNGID
# Sample data
data = [
        (101, "Mary", 102),
        (102, "Ravi", None),
        (103, "Raj", 102),
        (104, "Pete", 103),
        (105, "Prasad", 103),
        (106, "Ben", 103)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

df2 = df.select("MNGID").dropDuplicates()
df2.show()

# Show the DataFrame
df.show()

manager = df.join(df2, df["EMPID"] == df2["MNGID"], "inner").select("EMPID", "EMNAME")

manager.show()

countdf.show()

result = df.join(countdf, df["id"] == countdf["Manager"], "inner").where("count > 4")
result.show()

--------------------------------------------------------------

How to get the minimum, 25th percentile, median, 75th, and max of a numeric column

data = [("A", 10), ("B", 20), ("C", 30), ("D", 40), ("E", 50), ("F", 15), ("G", 28), ("H", 54), ("I", 41), ("J", 86)]
df = spark.createDataFrame(data, ["Name", "Age"])

df.show()

df.select(max("Age"), min("Age"), median("Age")).show()

quantiles = df.approxQuantile("Age", [0.25, 0.75], 0.01)

print("25th percentile: ", quantiles[0])

print("75th percentile: ", quantiles[1])

|Name|Age|
+----+---+
|   A| 10|
|   B| 20|
|   C| 30|
|   D| 40|
|   E| 50|
|   F| 15|
|   G| 28|
|   H| 54|
|   I| 41|
|   J| 86|
+----+---+

+--------+--------+-----------+
|max(Age)|min(Age)|median(Age)|
+--------+--------+-----------+
|      86|      10|       35.0|
+--------+--------+-----------+

25th percentile:  20.0
75th percentile:  50.0

---------------------------------------------------------------------------------------------------------------------------------------------------

How to get frequency counts of unique items of a column?

data = [
        Row(name='John', job='Engineer'),
        Row(name='John', job='Engineer'),
        Row(name='Mary', job='Scientist'),
        Row(name='Bob', job='Engineer'),
        Row(name='Bob', job='Engineer'),
        Row(name='Bob', job='Scientist'),
        Row(name='Sam', job='Doctor'),
]

# create DataFrame
df = spark.createDataFrame(data)

# show DataFrame
df.show()

# To count by entire row
df.groupby("name", "job").agg(count("job")).show()

# To count the no of jobs
df.groupby("job").agg(count("job")).show()

or

df.groupby("job").count().show()

---------------------------------------------------------------------------------------------------------------------------------------------------

How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?
Difficulty Level: L3

Input

from pyspark.sql import Row

# Sample data
data = [
Row(name='John', job='Engineer'),
Row(name='John', job='Engineer'),
Row(name='Mary', job='Scientist'),
Row(name='Bob', job='Engineer'),
Row(name='Bob', job='Engineer'),
Row(name='Bob', job='Scientist'),
Row(name='Sam', job='Doctor'),
]

# create DataFrame
df = spark.createDataFrame(data)

# show DataFrame
df.show()

# Get the top 2 most frequent jobs
top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()

# Replace all but the top 2 most frequent jobs with 'Other'
df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))

# show DataFrame
df.show()

or

df.withColumn("job", expr("case when job = 'Doctor' then 'Doctor' when job = 'Engineer' then 'Engineer' else 'Other' end")).show()

---------------------------------------------------------------------------------------------------------------------------------------------------



