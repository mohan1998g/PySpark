# write a sql query to find manager name with atleast 5 reporting employees
# Define the schema
schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("Name", StringType(), True),
        StructField("Dept", StringType(), True),
        StructField("Manager", IntegerType(), True)
])

# Sample data
data = [
        (101, "John", "A", None),
        (102, "Dan", "A", 101),
        (103, "James", "A", 101),
        (104, "Amy", "A", 101),
        (105, "Anne", "A", 101),
        (106, "Ron", "B", 101)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

# Show the DataFrame
df.show()

countdf = df.groupby("Manager").agg(count("Manager").alias("count"))

--------------------------------------------------------------
# first given is input MNGID = EMPID ,we need to get only manager details MNGID
# Define the schema
schema = StructType([
        StructField("EMPID", IntegerType(), True),
        StructField("EMNAME", StringType(), True),
        StructField("MNGID", IntegerType(), True)
])
# first given is input MNGID = EMPID ,we need to get only manager details MNGID
# Sample data
data = [
        (101, "Mary", 102),
        (102, "Ravi", None),
        (103, "Raj", 102),
        (104, "Pete", 103),
        (105, "Prasad", 103),
        (106, "Ben", 103)
]

# Create DataFrame
df = spark.createDataFrame(data, schema)

df2 = df.select("MNGID").dropDuplicates()
df2.show()

# Show the DataFrame
df.show()

manager = df.join(df2, df["EMPID"] == df2["MNGID"], "inner").select("EMPID", "EMNAME")

manager.show()

countdf.show()

result = df.join(countdf, df["id"] == countdf["Manager"], "inner").where("count > 4")
result.show()

--------------------------------------------------------------

How to get the minimum, 25th percentile, median, 75th, and max of a numeric column

data = [("A", 10), ("B", 20), ("C", 30), ("D", 40), ("E", 50), ("F", 15), ("G", 28), ("H", 54), ("I", 41), ("J", 86)]
df = spark.createDataFrame(data, ["Name", "Age"])

df.show()

df.select(max("Age"), min("Age"), median("Age")).show()

quantiles = df.approxQuantile("Age", [0.25, 0.75], 0.01)

print("25th percentile: ", quantiles[0])

print("75th percentile: ", quantiles[1])

|Name|Age|
+----+---+
|   A| 10|
|   B| 20|
|   C| 30|
|   D| 40|
|   E| 50|
|   F| 15|
|   G| 28|
|   H| 54|
|   I| 41|
|   J| 86|
+----+---+

+--------+--------+-----------+
|max(Age)|min(Age)|median(Age)|
+--------+--------+-----------+
|      86|      10|       35.0|
+--------+--------+-----------+

25th percentile:  20.0
75th percentile:  50.0

---------------------------------------------------------------------------------------------------------------------------------------------------

How to get frequency counts of unique items of a column?

data = [
        Row(name='John', job='Engineer'),
        Row(name='John', job='Engineer'),
        Row(name='Mary', job='Scientist'),
        Row(name='Bob', job='Engineer'),
        Row(name='Bob', job='Engineer'),
        Row(name='Bob', job='Scientist'),
        Row(name='Sam', job='Doctor'),
]

# create DataFrame
df = spark.createDataFrame(data)

# show DataFrame
df.show()

# To count by entire row
df.groupby("name", "job").agg(count("job")).show()

# To count the no of jobs
df.groupby("job").agg(count("job")).show()

or

df.groupby("job").count().show()

---------------------------------------------------------------------------------------------------------------------------------------------------

How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?
Difficulty Level: L3

Input

from pyspark.sql import Row

# Sample data
data = [
Row(name='John', job='Engineer'),
Row(name='John', job='Engineer'),
Row(name='Mary', job='Scientist'),
Row(name='Bob', job='Engineer'),
Row(name='Bob', job='Engineer'),
Row(name='Bob', job='Scientist'),
Row(name='Sam', job='Doctor'),
]

# create DataFrame
df = spark.createDataFrame(data)

# show DataFrame
df.show()

# Get the top 2 most frequent jobs
top_2_jobs = df.groupBy('job').count().orderBy('count', ascending=False).limit(2).select('job').rdd.flatMap(lambda x: x).collect()

# Replace all but the top 2 most frequent jobs with 'Other'
df = df.withColumn('job', when(col('job').isin(top_2_jobs), col('job')).otherwise('Other'))

# show DataFrame
df.show()

or

df.withColumn("job", expr("case when job = 'Doctor' then 'Doctor' when job = 'Engineer' then 'Engineer' else 'Other' end")).show()

---------------------------------------------------------------------------------------------------------------------------------------------------

How to Drop rows with NA values specific to a particular column?
Difficulty Level: L1

input

# Assuming df is your DataFrame
df = spark.createDataFrame([
("A", 1, None),
("B", None, "123" ),
("B", 3, "456"),
("D", None, None),
], ["Name", "Value", "id"])

df.show()
+----+-----+----+
|Name|Value| id|
+----+-----+----+
| A| 1|null|
| B| null| 123|
| B| 3| 456|
| D| null|null|
+----+-----+----+

df_2 = df.dropna(subset=['Value'])

df_2.show()How to create contigency table?
Difficulty Level: L1

Input

# Example DataFrame
data = [("A", "X"), ("A", "Y"), ("A", "X"), ("B", "Y"), ("B", "X"), ("C", "X"), ("C", "X"), ("C", "Y")]
df = spark.createDataFrame(data, ["category1", "category2"])

df.show()
+---------+---------+
|category1|category2|
+---------+---------+
| A| X|
| A| Y|
| A| X|
| B| Y|
| B| X|
| C| X|
| C| X|
| C| Y|
+---------+---------+
Show Solution
# Frequency
df.cube("category1").count().show()

# Contingency table
df.crosstab('category1', 'category2').show()

+---------+-----+
|category1|count|
+---------+-----+
| null| 8|
| A| 3|
| B| 2|
| C| 3|
+---------+-----+

+-------------------+---+---+
|category1_category2| X| Y|
+-------------------+---+---+
| A| 2| 1|
| B| 1| 1|
| C| 2| 1|
+-------------------+---+---+

df1 = df.groupby("category1", "category2").count()
df1.show()

pivot = df1.groupby("category1").pivot("category2").agg(first("count"))
pivot.show()

+---------+---------+-----+
|category1|category2|count|
+---------+---------+-----+
|        A|        X|    2|
|        B|        Y|    1|
|        C|        X|    2|
|        C|        Y|    1|
|        B|        X|    1|
|        A|        Y|    1|
+---------+---------+-----+

+---------+---+---+
|category1|  X|  Y|
+---------+---+---+
|        B|  1|  1|
|        C|  2|  1|
|        A|  2|  1|
+---------+---+---+

---------------------------------------------------------------------------------------------------------------------------------------------------

How to get the names of DataFrame objects that have been created in an environment?

dataframe_names = [name for name, obj in globals().items() if isinstance(obj, pyspark.sql.DataFrame)]

for name in dataframe_names:
print(name)

---------------------------------------------------------------------------------------------------------------------------------------------------

How to convert the categorical string data into numerical data or index?

from pyspark.ml.feature import StringIndexer

# Create a sample DataFrame
data = [('cat',), ('dog',), ('mouse',), ('fish',), ('dog',), ('cat',), ('mouse',)]
df = spark.createDataFrame(data, ["animal"])

df.show()

# Initialize a StringIndexer
indexer = StringIndexer(inputCol='animal', outputCol='animalIndex')

# Fit the indexer to the DataFrame and transform the data
indexed = indexer.fit(df).transform(df)
indexed.show()

+------+-----------+
|animal|animalIndex|
+------+-----------+
| cat| 0.0|
| dog| 1.0|
| mouse| 2.0|
| fish| 3.0|
| dog| 1.0|
| cat| 0.0|
| mouse| 2.0|
+------+-----------+

---------------------------------------------------------------------------------------------------------------------------------------------------

How to restrict the PySpark to use the number of cores in the system?

from pyspark import SparkConf, SparkContext

conf = SparkConf()
conf.set("spark.executor.cores", "2") # set the number of cores you want here
sc = SparkContext(conf=conf)

---------------------------------------------------------------------------------------------------------------------------------------------------

How to View PySpark Cluster Details?

print(spark.sparkContext.uiWebUrl)

http://DESKTOP-UL3QT3E.mshome.net:4040

---------------------------------------------------------------------------------------------------------------------------------------------------

58. How to View PySpark Cluster Configuration Details?

# Print all configurations
for k,v in spark.sparkContext.getConf().getAll():
print(f"{k} : {v}")

spark.app.name : PySpark 101 Exercises
spark.driver.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED
spark.app.startTime : 1684510291553
spark.app.id : local-1684510293468
spark.driver.host : DESKTOP-UL3QT3E.mshome.net
spark.executor.id : driver
spark.sql.warehouse.dir : file:/C:/Users/RajeshVaddi/Documents/MLPlus/6_PySpark%20101%20Exercises/spark-warehouse
spark.driver.port : 50321
spark.app.submitTime : 1684510291319
spark.rdd.compress : True
spark.executor.extraJavaOptions : -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED
spark.serializer.objectStreamReset : 100
spark.master : local[*]
spark.submit.pyFiles :
spark.submit.deployMode : client
spark.ui.showConsoleProgress : true

---------------------------------------------------------------------------------------------------------------------------------------------------

How to find installed location of Apache Spark and PySpark?

import findspark
findspark.init()

print(findspark.find())

import os
import pyspark

print(os.path.dirname(pyspark.__file__))

C:\Users\subhash_narkedamilli\Projects\sudheer\venv\lib\site-packages\pyspark
C:\Users\subhash_narkedamilli\Projects\sudheer\venv\lib\site-packages\pyspark

---------------------------------------------------------------------------------------------------------------------------------------------------

