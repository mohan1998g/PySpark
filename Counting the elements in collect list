data = [
    ("a", [1, 1, 1, 3]),
    ("b", [1, 2, 3, 4]),
    ("c", [1, 1, 1, 1, 4]),
    ("d", [3])
]
df = spark.createDataFrame(data, ["name", "rank"])
df.show()

explodedf = df.withColumn("rank", explode(col("rank")))
explodedf.show()

filtdf = explodedf.filter(col("rank") == 1)
filtdf.show()

countdf = filtdf.groupBy("name").agg(count("*").alias("count")).orderBy(col("count").desc())
countdf.show()


finaldf = countdf.select(col("name")).first()[0]
print(finaldf)
